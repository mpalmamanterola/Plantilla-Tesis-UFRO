\chapter{Fundamentos Teóricos}
\label{C2}
\section{Reducción de escala}
La reducción de escala (en inglés, \textit{Downscalling}) es un procedimiento utilizado para inferir 
información de alta resolución, como por ejemplo datos satelitales, a partir de variables de baja 
resolución. Esta técnica se basa en enfoques dinámicos o estadísticos comúnmente utilizados en varias 
disciplinas, especialmente la meteorología, la climatología y la teledetección. El término reducción 
de escala generalmente se refiere a un aumento en la resolución espacial, pero a menudo se usa para la 
resolución temporal.
%
%
%
%
\section{Datos georreferenciados}
La georreferenciación es el uso de coordenadas de un mapa para asignar una ubicación espacial a entidades
cartográficas. Los elementos del conjunto de datos tienen asignados una ubicación geográfica y una extensión específicas
que permiten situarlos en la superficie terrestre o cerca de ella.

La correcta descripción de la ubicación requiere un marco para definir ubicaciones del mundo real. Un sistema de coordenadas geográficas
se utiliza para asignar ubicaciones georreferenciadas a los datos. El sistema de coordenadas latitud-longitud global es uno de los marcos.
Otro marco muy utilizado resulta ser el sistema de coordenadas cartesianas que surge a partir del marco global a través de ciertas transformaciones
en el cuerpo de los números complejos ($\mathbb{C}$).

La ventaja de trabajar con datos que contengan georreferencias, es que con estos pueden generarse grillas cuadriculadas regulares, es decir, mediante una distancia arbitraria (generalmente determinada en grados $[^{\circ}]$)
es posible generar observaciones equidistantes entre sí lo cual entrega regularidad para remuestrear y analizar datos.

    \subsection{Longitud y Latitud}
    Un método para describir la posición de una observación en alguna de las superficies (comúnmente llamadas \textit{capas}) consiste en utilizar 
    mediciones esféricas de latitud y longitud. Estas son mediciones de los ángulos (en grados) desde el centro de la Tierra hasta un punto de una determinada
    capa. Este tipo de coordenadas se denominan coordenadas geográficas.

    La \textbf{longitud} mide ángulos en una dirección este-oeste. Las mediciones de longitud usualmente se basan en el meridiano de Greenwich, que es una línea
    imaginaria que realiza un recorrido desde el Polo Norte hasta el Polo Sur, en este meridiano la longitud es 0$^{\circ}$. El oeste del meridiano de Greenwich por lo general 
    se registra como longitud negativa y el este, como longitud positiva.

    La \textbf{latitud} mide ángulos en una dirección norte-sur. Las mediciones de latitud comienzan desde la línea del Ecuador, que es la máxima circunferencia perpendicular
    al eje de rotación del planeta Tierra, en este paralelo la latitud es 0$^{\circ}$. El norte de la línea ecuatorial por lo general 
    se describe como latitud positiva y al sur, como latitud negativa.

    Si bien la longitud y latitud se pueden ubicar en posiciones exactas de la superficie de la Tierra, estas no proporcionan
    unidades de medicion uniformes de longitud y latitud. Solo a lo largo del ecuador la distancia que representa un grado de longitud se aproxima a la distancia
    en un grado de latitud. Esto se debe a que el ecuador es el único paralelo tan extenso como cualquier meridiano, ya que si nos alejamos del ecuador, el tamaño circunferencial de
    los paralelos disminuye hasta transformarse en un punto en los polos donde los meridianos convergen.
%    
%
%
%
\section{Error}
En modelos predictivos, la diferencia entre un valor real y una estimación, o aproximación se conoce como error, por lo tanto, mientras más pequeño sea
la estimación se vuelve más verídica. En otras palabras, el error es utilizado para medir el rendimiento de un modelo predictivo y así poder
realizar comparaciones entre diferentes estimadores con el fin de seleccionar el mejor.
Hay muchas maneras de medir el error de acuerdo a las necesidades que se dispongan.


En un modelo de clasificación el error y la exactitud van de la mano, ya que el primero se puede definir como el porcentaje de clasificaciones erradas que realiza el modelo 
y la exactitud es el porcentaje de aciertos de clasificación.
En cambio, en un modelo regresivo no podemos establecer una relacion tan directa ya que la predicción se encuentra en un dominio continuo, por lo tanto, si se mide el 
error de la misma forma que para un modelo de clasificación, la mayoría de las veces entregará un valor muy cercano al 100\%, es por esto que se recurre a otras maneras de medirlo: 
el \textbf{error medio absoluto} y el \textbf{error cuadrático medio}, en inglés \textit{mean absolute error (MAE)} y \textit{mean squared error (MSE)}, respectivamente.

    
    \subsection{Error medio absoluto}
    El error medio absoluto, o \textit{MAE}, es una métrica de evaluación utilizada
    en modelos regresivos. El error medio absoluto de un modelo respecto a un conjunto de prueba (o \textit{test}) resulta ser, como su nombre lo indica, la media de los
    valores absolutos de los errores de predicción individuales en todas las instancias del conjunto \textit{test} \cite{5}.
    
    \begin{equation}\label{eqn:mae}
        mae = \frac{1}{n} \sum_{i=1}^n|~y_i - \bar{y}_i~|
    \end{equation}

    En la ecuación \ref{eqn:mae}, $y_i$ es el valor real de la variable objetivo de la instancia $i$, $\bar{y}_i$ es la predicción de la $i$-ésima instancia en el mismo conjunto
    y $n$ es la cantidad de elementos en el conjunto de prueba.

    \subsection{Error cuadrático medio}
    El error cuadrático medio, o \textit{MSE}, es una métrica de evaluación utilizada
    en modelos regresivos. El error cuadrático medio de un modelo respecto al conjunto de prueba es la media del cuadrado de los errores de predicción 
    en cada instancia del conjunto \cite{4}.

    \begin{equation}\label{eqn:mse}
        mse = \frac{1}{n} \sum_{i=1}^n(y_i - \bar{y}_i)^2
    \end{equation}

    En \ref{eqn:mse}, $y_i$ es el valor real de la variable objetivo de la instancia $i$, $\bar{y}_i$ es la predicción de la $i$-ésima instancia en el mismo conjunto
    y $n$ es la cantidad de elementos en el conjunto de prueba.
%   
%
%
%
\section{Bootstrap}
En el contexto de estadísticas y ciencia de datos, el \textit{bootstrapping} es un método para inferir resultados
de una población a partir de resultados obtenidos de una colección aleatoria igual o más pequeña de esa población. Por tanto, 
el método requiere la noción de \textit{bootstrap sample}, la cual se define como una \textbf{muestra aleatoria con reposición}
\cite{17}. Esto significa que, luego de que una observación de los datos ha sido elegida del conjunto para obtener la muestra, este sigue 
estando disponible para una selección posterior. La \textit{muestra de bootstrap} debe ser del mismo tamaño que el conjunto de datos original \cite{18}, debido a esto, 
algunas muestras serán consideradas múltiples veces mientras que otras no serán seleccionadas. Las muestras que no han sido seleccionadas 
se conocen como \textit{out of bag samples}, o muestras fuera de la bolsa, ya que estas no se incluirán dentro del modelo.

%
%
%
%
\section{Métodos basados en árboles}

Los métodos basados en árboles son algoritmos de \textit{Machine Learning} del tipo supervisado donde la  
clasificación o predicción se puede ver como una división de los datos en un conjunto de rectángulos \cite{13} y en
cada uno se ajusta un modelo tomando una decisión de acuerdo a una serie de consultas, el modelo
aprende la respuesta a una serie de interrogantes para inferir la etiqueta de clase de los
ejemplos (clasificación) o también la predicción de números reales (regresión). En otras palabras, 
Los modelos basados en árboles involucran \textit{estratificación} o \textit{segmentación} del 
espacio de predicción dentro de un número de regiones simples, donde la predicción se lleva a cabo 
en base a la media o la moda de las observaciones de entrenamiento de la región a la que pertenece \cite{14}, según las necesidades.

Los métodos basados en reglas y los métodos basados en árboles son herramientas populares debido a varias razones,
una de ellas es debido a su construcción lógica \cite{18}, la cual permite manejar eficientemente muchos tipos de predictores
, por ejemplo predictores continuos (regresión) o categóricos sin la necesidad de procesar datos. Además,
estos modelos logran lidiar de forma eficiente los valores faltantes e implicitamente llevan a cabo una selección de características,
lo cual resulta importante para problemas de modelación de la vida real.

Estos modelos poseen ciertas falencias, dos de ellas en particular bien estudiadas:
\newpage
\begin{enumerate}
    \item La \textbf{inestabiliad} del modelo, es decir, ligeros cambios en las variables explicativas 
          pueden llevar a grandes cambios en la estructura del árbol.
    \item El \textbf{Rendimiento predictivo menor} que el optimal, esto debido a que el modelo define estructuras rectangulares y si 
          la relación entre las variables explicativas y la variable respuesta no puede ser definida adecuadamente por
          subespacios rectangulares de los predictores, entonces el modelo tendrá un mayor error de predicción.
\end{enumerate} 


Para lidiar con estos inconvenientes, investigadores desarrollaron conjuntos de modelos que combinan una cantidad fija de árboles
y una variación del \textit{Bootstrap} para mejorar el rendimiento.

    \subsection{Árboles de Regresión}
    Existen muchas formas de construir árboles de regresión, una de las más antiguas y más utilizadas
    resulta ser el árbol de clasificación y regresión \textit{CART}, por sus siglas en inglés 
    \textit{Clasification And Regresion Tree}, el cual comienza buscando todos los posibles valores de cada predictor para poder encontrar a cada uno,
    o en otras palabras, poder conocer su comportamiento y determinar el valor que divide los datos en dos subconjuntos, de modo que la suma de errores 
    cuadrados de cada subconjunto sea minimizada \cite{18}. Entonces, si denotamos como $X$ el conjunto de todos los datos y como $X_1 \text{ y } X_2$ a la
    partición de este mediante \textit{CART}, entonces el error que se busca minimizar es la suma de \textit{MSE} (ec. \ref{eqn:mse}) en cada partición, esto es:
    \begin{equation*} 
        SSE = \sum_{i\in X_1}(y_i-\bar{y}_1)^2+ \sum_{j\in X_2}(y_j-\bar{y}_2)^2
    \end{equation*}
    donde $y_i$ y $y_j$ representan la predicción del modelo y $\bar{y}$ representa la media de la predicción en cada subconjunto.

    Este método utiliza una división a partir del conjunto entero de datos aumentando la profundidad del modelo. En la práctica el proceso continúa
    dentro de los conjuntos $X_1$ y $X_2$ hasta que el número de divisiones en la muestra cae debajo de cierto umbral, ahí culmina el proceso de
    \textit{paso de crecimiento del árbol} \cite{18} o también llamada profundidad del árbol.
    Por esta razón este método es comúnmente conocido como \textbf{partición recursiva}. Sin duda el mayor de los problemas es debido a la inestabilidad que conduce a una alta varianza. 
    A menudo un pequeño cambio en los datos podría resultar en una serie de divisiones muy diferente, precarizando la predicción. La mayor razón de inestabilidad es la naturaleza 
    jerárquica del proceso; El efecto de un pequeño error en una de las primeras divisiones es propagada hacia abajo por todas las divisiones siguientes \cite{13}.
%
%
%
%
\section{Aprendizaje de conjunto}
En los años 90's comenzaron a emerger métodos que combinan las predicciones de muchos modelos para generar una nueva, estas se llamaron \textbf{técnicas de conjunto},
o en inglés \textit{ensemble techniques} \cite{18}.

El aprendizaje en conjunto (\textit{ensemble learning}) se refiere a los procedimientos empleados para entrenar múltiples máquinas de aprendizaje y combinar sus resultados, tratándolos como un ``comité"$~$de tomadores de decisiones. 
El principio es que la decisión del comité, con las predicciones individuales combinadas adecuadamente, debería tener una mejor precisión general, en promedio, que cualquier miembro individual del comité. 
Numerosos estudios empíricos y teóricos han demostrado que los modelos de conjuntos muy a menudo alcanzan una mayor precisión que los modelos individuales.
Los miembros del conjunto pueden estar prediciendo números con valores reales, etiquetas de clase, probabilidades, clasificaciones, agrupaciones o cualquier otra cantidad. Por lo tanto, 
sus decisiones se pueden combinar mediante muchos métodos, incluidos los métodos de promediación, votación y probabilísticos. La mayoría de los métodos de aprendizaje por conjuntos son genéricos, 
aplicables en varios tipos de modelos y tareas de aprendizaje automático \cite{6}.


\section{Bagging}
\textit{Bagging}, es una palabra compuesta por los términos en inglés \textit{\textbf{B}ootstrap \textbf{agg}regat\textbf{ing}}, 
debido a que consite en un método que utiliza el \textit{bootstrap} en conjunto con cualquier regresión o clasificación
para generar múltiples versiones de un predictor y usarlo para obtener un predictor ``agregado'' \cite{22}; Cada modelo en el conjunto es utilizado para generar una predicción de una observación en particular 
y luego se promedian entre ellas para obtener una predicción \textit{bagged} del modelo .
Este método fue introducido y propuesto por Leo Breiman y es una de las técnicas
de conjunto más nuevas. El pseudocódigo de este método es el siguiente:

\begin{algorithm}[H]
    \caption{Bagging}

    \begin{algorithmic}[1]
        \FOR{$i=1\text{ to }m$} 
        \STATE{Generar una muestra de \textit{bootstrap} de los datos originales}
        \STATE{Entrenar el modelo en la muestra generada en 2}
        \ENDFOR

    \end{algorithmic}
\end{algorithm}

\textit{Bagging} funciona mejor con modelos que sean inestables, es decir, aquellos que producen diferentes
patrones de generalización con pequeños cambios a los datos de entrenamiento. Por lo tanto, el \textit{bagging} no tiende a funcionar bien con modelos lineales \cite{19}.

Una de las ventajas de esta metodología es la reducción de varianza de la predicción a través del processo de agregación. Además, para aquellos modelos que producen una predicción inestable,
o de alta varianza como lo son los árboles de regresión hace que éstos sean más estables \cite{18} y por tanto, más precisos.

La desventaja más clara del método resulta ser el alto costo computacional, ya que este aumentará a medida que el número de muestras
de \textit{bootstrap} aumenten. Éste problema es solucionable si se posee acceso a cómputo paralelo debido a que el \textit{bagging} es fácilmente paralelizable \cite{18}.
%
%
%
%
\section{Modelo del bosque aleatorio}
Los bosques aleatorios son un \textit{ensemble} de predictores de árboles de modo que cada árbol depende de los valores de un vector aleatorio muestreado de forma independiente (\textit{bootstrap sample}) y con la misma distribución para cada uno de ellos. 
El error de generalización para los bosques converge estable asintóticamente a un límite a medida que aumenta el número de árboles en el bosque, además, depende entre otras cosas, de la correlación entre ellos. Usar una selección aleatoria de características para dividir cada nodo produce
tasas de error que se comparan favorablemente con otros modelos como Adaboost, pero son más sólidas con respecto al ruido, error de estimaciones internas,
fuerza y correlación, y se utilizan para mostrar la respuesta al aumento del número de características utilizadas
en la división. Las estimaciones internas también se utilizan para medir la importancia de la variable \cite{25}.

La ventaja que más resalta del modelo del bosque aleatorio recae en que es un modelo \textbf{escalable} y además fácil de usar. La idea detrás es promediar múltiples, además de profundos, 
árboles de decisión los cuales de forma individual sufren de una alta varianza. Utilizando este conjunto de árboles es posible obtener una mejor generalización en la predicción y además de ser mucho menos
susceptible al sobreajuste, es decir, ajustarse de sobremanera al conjunto de prueba \cite{24}.

El modelo en cuestión se resume en el siguiente pseudocódigo:

\newcommand{\INDSTATE}[1][1]{\STATE\hspace{#1\algorithmicindent}}

\begin{algorithm}[h]
    \caption{Modelo Random Forest}

    \begin{algorithmic}
            \STATE 1. \textbf{Construír un \textit{bootstrap sample}} de tamaño $n$.
            \STATE 2. \textbf{Generar un árbol de clasificación y regresión} para la muestra de \textit{bootstrap} y en cada nodo:

            \INDSTATE a) Aleatoriamente \textbf{seleccionar $d$ características} sin reposición.

            \INDSTATE b) \textbf{Dividir el nodo} utilizando las características que proporcionan la mejor división de\INDSTATE acuerdo con la función objetivo(maximización de la ganancia de información, minimi-\INDSTATE zación del \textit{RMSE}, entre otras).
            
            \STATE 3. \textbf{Repetir} los pasos 1. y 2. $k$ veces.
            \STATE 4. \textbf{Agregar la predicción de cada uno de los $k$ árboles} para obtener una predicción \textit{bagged}, es decir, una predicción que combina de alguna forma, por ejemplo el promedio, 
            de las predicciones de cada árbol por sí solo.
    \end{algorithmic}
\end{algorithm}



%
%
%
%
\section{Interpolación bilinear}
La interpolación bilinear es una extensión de la interpolación lineal en una variable con el objetivo de interpolar funciones de dos variables en una malla regular, por ejemplo coordenadas geográficas o imágenes.
La idea principal es realizar una interpolación lineal en una dirección y luego en la otra. Aunque las operaciones en cada dirección son lineales, la interpolación bilinear es no lineal. 

Si se tiene la observación en los 4 vecinos más cercanos de la grilla a un determinado punto $(n_1, n_2)$, digamos 
$f(n_{10}$,$n_{20})$, $f(n_{11},n_{21})$, $f(n_{12},n_{22})$, $f(n_{13},n_{23})$, entonces en la coordenada $(n_1, n_2)$ se define la 
función bilinear $g(n_1, n_2)$ que aproxima la observación como
\begin{equation}
    g(n_1, n_2) = A_0 + A_1 n_1 + A_2n_2 + A_3n_1n_2
\end{equation}

Los llamados ``pesos bilineares''\cite{26} $A_0,~A_1,~A_2\text{ y }A_3$, son calculados resolviendo el siguiente sistema de ecuaciones:
\begin{equation}\label{interp2d}
    \begin{bmatrix}
        A_0 \\
        A_1 \\
        A_2 \\
        A_3 \\
    \end{bmatrix}
    =
    \begin{bmatrix}
        1 & n_{10} & n_{20} & n_{10}n_{20} \\
        1 & n_{11} & n_{21} & n_{11}n_{21} \\
        1 & n_{12} & n_{22} & n_{12}n_{22} \\
        1 & n_{13} & n_{23} & n_{13}n_{23} 
    \end{bmatrix}^{-1}
    \begin{bmatrix}
        f(n_{10},n_{20}) \\
        f(n_{11},n_{21}) \\
        f(n_{12},n_{22}) \\
        f(n_{13},n_{23}) \\
    \end{bmatrix}
\end{equation}
Por lo tanto $g(n_1,n_2)$ está definido para ser una combinación linear de los 4 vecinos más cercanos de una determinada observación.
%
%
%
%
\section{Series de tiempo}
Una serie de tiempo consite en un conjunto de observaciones $x_t$, cada una es registrada en un tiempo fijo $t$.
Una serie de tiempo a \textbf{tiempo discreto} es cuando el intervalo de tiempos $T_0$ en donde se llevan a cabo las observaciones
es discreto, por ejemplo, cuando se realizan en intervalos de tiempos fijos (mediciones diarias, mensuales, anuales, etc.). 
En cambio, las series de tiempo a \textbf{tiempo continuo} son obtenidas cuando las observaciones recopiladas se encuentran medidas sobre un intervalo
contínuo de tiempo, por ejemplo $T_0=[0,1]$ \cite{21}. Una serie de tiempo representa la relación entre dos variables, el tiempo es la primera 
y la segunda es cualquier variable cuantitativa. No es necesario que la relación muestre siempre incremento en el cambio de la variable con referencia al tiempo. 

Los usos más comunes para realizar estudios de series de tiempo son variados, entre ellos se encuentran la predicción de comportamientos
futuros de la variable cuantitativa basada en las observaciones o registro histórico, la planificación de negocios en base a comparación del 
rendimiento real con el esperado, comparar comportamientos entre variables para cierto lugar geográfico, entre muchas otras. 


    \subsection{Componentes de las series de tiempo}
    
    Las diversas razones o fuerzas que afectan los valores de una observación en una serie de tiempo son sus componentes. 
    Las cuatro categorías de las componentes de las series de tiempo son la \textbf{tendencia}, la \textbf{variación estacional}, la \textbf{variación cíclica} 
    y los \textbf{movimientos irregulares} o también llamados residuos.

    \subsubsection{Tendencia}
    Muestra la tendencia general de los datos a aumentar o disminuir durante un largo período de tiempo. Una tendencia es de media suave, general y 
    a largo plazo. No siempre es necesario que el aumento o la disminución sea en la misma dirección a lo largo del período de tiempo dado.
    Se observa que las tendencias pueden aumentar, disminuir o mantenerse estables en diferentes tramos de tiempo. Pero la tendencia general debe ser al alza, a la baja o estable. 
    Usualmente se refiere a un ``cambio de dirección en la tendencia'' cuando el componente varía de una tendencia incremental a una decreciente o viceversa \cite{20}.

    \subsubsection{Estacionalidad y Cíclica}
    
    La \textbf{estacionalidad} son las fuerzas rítmicas que operan de manera regular y periódica en un lapso de menos de un año. Tienen el mismo o casi el mismo patrón durante un período de 12 meses. 
    Además, siempre es obtenida a partir de un periodo fijo y conocido de tiempo \cite{20}, ya sea cada hora, día, semana, trimestre o mes.
    
    Estas variaciones entran en juego debido a las fuerzas naturales o por convenciones hechas por el hombre. Las diversas estaciones o condiciones climáticas juegan un papel 
    importante en las variaciones estacionales. 

    Las variaciones en una serie de tiempo que operan en un lapso de más de un año son las \textbf{variaciones cíclicas}. Este movimiento oscilatorio tiene un período de más de un año que se conoce como ciclo.
    Este movimiento a veces se denomina ``ciclo económico''.

    Es un ciclo de cuatro fases que comprende las fases de prosperidad, recesión, depresión y recuperación. La variación cíclica puede ser regular no periódica.

    \subsubsection{Residuos}
    
    Hay otro factor que provoca la variación en la variable en estudio. No son variaciones regulares y son puramente aleatorias o irregulares. 
    Estas fluctuaciones son imprevistas, incontrolables, impredecibles y erráticas. Estas fuerzas son terremotos, guerras, inundaciones, hambrunas y cualquier otro desastre.

    \subsection{Descomposición de series de tiempo}
    Los datos en series de tiempo pueden exhibir una gran variedad de patrones y es útil categorizarlos y estudiar
    comportamientos que se puedan apreciar en ellas. Otras veces es útil intentar dividir la serie de tiempo en varios componentes,
    cada uno representando uno de los patrones subyacentes \cite{20}.

    La forma más sencilla y popular de descomponer una serie de tiempo es pensar que la serie $y_t$ se encuentra compuesta por 
    3 componentes: la componente estacional, la componente tendencial-cíclica y la componente residual que resulta ser
    todo lo restante para completar la serie \cite{20}.

    Si se asume que la serie de tiempo se puede descomponer de forma aditiva, entonces:
    \begin{equation}
        y_t = S_t + T_t + E_t
     \end{equation}
        
     En la ecuación,  $y_t$ es la observación o variable cuantitativa en el instante $t$, $S_t$ es la componente estacional, $T_t$ es la componente tendencial cíclica
     y $E_t$ es la serie de tiempo residual o también llamado \textit{irregular} \cite{20}, todas ellas observadas en el mismo instante $t$.

     De manera alternativa, una descomposición multiplicativa de la serie de tiempo puede escribirse como:
     \begin{equation}
        y_t = S_t 
        \times T_t 
        \times E_t
     \end{equation}
     la cual es equivalente a una doscomposición logarítimca de la serie, ya que:
     \begin{equation*}
        y_t = S_t 
        \times T_t 
        \times E_t
        \iff
        \log y_t =  \log S_t +
        \log T_t +
        \log E_t
     \end{equation*}

     El modelo aditivo resulta más apropiado si la magnitud de las fluctuaciones estacionales o las variaciones
     a través de la componente tendencial-cíclica no varía con el nivel de la serie de tiempo.


     \subsection{Comparación de series de tiempo}
     En la práctica, es frecuente enfrentar situaciones
     en las que se requiere comparar dos o más series temporales, o analizar un gran
     número de estas series para separarlas en grupos tan homogéneos como sea posible \cite{27}.
     Normalmente, se generan bases de datos temporales que luego se estudian para identificar concordancias y disimilitudes.    

     Una de las medidas convencionales de proximidad es la distancia euclídea. Si bien esta medida es muy utilizada, ignora la interdependencia entre
     las mediciones de una serie temporal y basa la similitud entre dos series únicamente en la proximidad entre valores observados en los mismos instantes de
     tiempo \cite{27}.
     \subsubsection{Índice de correlación de Pearson}
     La correlación temporal, o índice de correlación de Pearson, es un índice que tiene por objetivo medir la similitud lineal en las tasas de crecimiento de dos series en períodos de
     tiempo de la forma $[t_i; t_{i+1}]$. Se define como:
     \begin{equation}\label{eqn:cort}
        CorrT(S_1,S_2) = \frac{\sum_{j=1}^p\sum_{i=1}^p(u_i-u_j)(v_i-v_j)} {\sqrt{\sum_{i=1}^p(u_i-u_j)^2}\sqrt{\sum_{i=1}^p(v_i-v_j)^2}}
     \end{equation}

     En la ecuación \ref{eqn:cort}, $S_1$ y $S_2$ son las series temporales a comparar las cuales poseen observaciones $ u_1,\dots,u_p $ y $ v_1,\dots,v_p$ respectivamente.

     El coeficiente de correlación temporal oscila en el intervalo $[-1,1]$. Si el coeficiente posee un valor de 1 quiere decir que en cualquier período de tiempo $[t_i,t_{i+1}]$ las series
     que se están comparando han aumentado o disminuído de manera simultánea con la misma tasa de creciemiento. En cambio, si alcanza el valor de -1 quiere decir que en cualquier periodo, si una de ellas ha
     aumentado, la otra ha disminuído o viceversa. Finalmente, si alcanza el valor de 0 significa que las tasas de crecimiento o decrecimiento son estocásticamente linealmente independientes \cite{27}.

     \subsubsection{Anomalías porcentuales}
     Una manera de poder realizar comparaciones en series de tiempo es mediante el cálculo de anomalías porcentuales. La gran ventaja que posee esta, es que el recorrido de la serie de tiempo
     se escala en valores porcentuales para determinar qué tanto varía una serie de tiempo respecto a su media. La fórmula para calcular las anomalías porcentuales es la siguiente:

    \begin{equation}\label{pct_anomalies}
        pct_t = 100 \cdot\frac{x_t-\mu}{\mu}
    \end{equation}
    Aquí, $x_t$ corresponde a cada componente temporal de la serie de tiempo y $\mu$ representa el promedio de toda la serie. 
